{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VETT-UwATzPx"
      },
      "source": [
        "# Medical RAG System - Document Retrieval and Question Answering\n",
        "\n",
        "This notebook demonstrates a Retrieval Augmented Generation (RAG) system for medical knowledge base queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boPA4l1WSiBs",
        "outputId": "f923c948-d5d7-4d3e-952c-eb23c86b5711"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain openai langchain-google-genai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWUDnCx0Tutm"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCSoDdXyTuq4"
      },
      "outputs": [],
      "source": [
        "# Load medical knowledge base\n",
        "loader = TextLoader(\"medical_data.txt\")\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF7eavqTTun7"
      },
      "outputs": [],
      "source": [
        "# Split documents into chunks for better retrieval\n",
        "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = splitter.split_documents(documents)\n",
        "print(f\"Created {len(docs)} document chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LZv_SN9TulU"
      },
      "outputs": [],
      "source": [
        "# Simple text-based retrieval system\n",
        "def simple_search(query, documents, top_k=3):\n",
        "    \"\"\"Keyword-based search function\"\"\"\n",
        "    query_words = query.lower().split()\n",
        "    \n",
        "    scores = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc_text = doc.page_content.lower()\n",
        "        score = sum(doc_text.count(word) for word in query_words)\n",
        "        scores.append((score, i, doc))\n",
        "    \n",
        "    scores.sort(reverse=True)\n",
        "    return scores[:top_k]\n",
        "\n",
        "print(\"Document retrieval system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug60M8ebW3Qp"
      },
      "outputs": [],
      "source": [
        "# API Key Setup (Optional - for enhanced responses)\n",
        "# Uncomment one of the following options to use API-based responses:\n",
        "\n",
        "# Option 1: OpenAI API\n",
        "# openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Option 2: Google Generative AI\n",
        "# google_api_key = getpass(\"Enter your Google API Key: \")\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "\n",
        "print(\"API setup ready (currently using simple text-based responses)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AprVQ6mBTuiq"
      },
      "outputs": [],
      "source": [
        "# Answer generation function\n",
        "def generate_answer(query, retrieved_docs):\n",
        "    \"\"\"Extract relevant sentences from retrieved documents\"\"\"\n",
        "    query_words = query.lower().split()\n",
        "    relevant_sentences = []\n",
        "    \n",
        "    for score, idx, doc in retrieved_docs:\n",
        "        sentences = doc.page_content.split('.')\n",
        "        for sentence in sentences:\n",
        "            if any(word in sentence.lower() for word in query_words):\n",
        "                relevant_sentences.append(sentence.strip())\n",
        "    \n",
        "    if relevant_sentences:\n",
        "        return \". \".join(relevant_sentences[:3]) + \".\"\n",
        "    else:\n",
        "        return \"No specific information found in the medical knowledge base.\"\n",
        "\n",
        "print(\"Answer generation system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Za4JtiNTuf4"
      },
      "outputs": [],
      "source": [
        "# Main QA System\n",
        "def medical_qa_system(question):\n",
        "    \"\"\"Complete QA system for medical queries\"\"\"\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    retrieved_docs = simple_search(question, docs, top_k=3)\n",
        "    \n",
        "    # Step 2: Generate answer\n",
        "    answer = generate_answer(question, retrieved_docs)\n",
        "    \n",
        "    return answer, retrieved_docs\n",
        "\n",
        "print(\"Medical QA system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_w2dyBaTudJ"
      },
      "outputs": [],
      "source": [
        "# Enhanced QA System with API Integration\n",
        "def enhanced_qa_system(question, use_api=False):\n",
        "    \"\"\"Enhanced QA system with optional API integration\"\"\"\n",
        "    retrieved_docs = simple_search(question, docs, top_k=3)\n",
        "    context = \"\\n\\n\".join([doc.page_content for score, idx, doc in retrieved_docs])\n",
        "    \n",
        "    if use_api and \"OPENAI_API_KEY\" in os.environ:\n",
        "        try:\n",
        "            import openai\n",
        "            client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "            \n",
        "            prompt = f\"\"\"Based on the medical information below, answer the question comprehensively.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=500,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            return response.choices[0].message.content, retrieved_docs\n",
        "        except Exception as e:\n",
        "            print(f\"API error: {e}. Using simple generation...\")\n",
        "    \n",
        "    elif use_api and \"GOOGLE_API_KEY\" in os.environ:\n",
        "        try:\n",
        "            import google.generativeai as genai\n",
        "            genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "            model = genai.GenerativeModel('gemini-pro')\n",
        "            \n",
        "            prompt = f\"\"\"Based on the medical information below, answer the question comprehensively.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text, retrieved_docs\n",
        "        except Exception as e:\n",
        "            print(f\"API error: {e}. Using simple generation...\")\n",
        "    \n",
        "    # Fallback to simple generation\n",
        "    return generate_answer(question, retrieved_docs), retrieved_docs\n",
        "\n",
        "print(\"Enhanced QA system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoDBlL7qTuaR"
      },
      "outputs": [],
      "source": [
        "# Demo: Test the Medical RAG System\n",
        "print(\"🏥 MEDICAL RAG SYSTEM DEMO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_questions = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How is hypertension treated?\",\n",
        "    \"What causes asthma?\",\n",
        "    \"What are the risk factors for heart disease?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. Question: {question}\")\n",
        "    answer, retrieved_docs = medical_qa_system(question)\n",
        "    print(f\"   Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}\")\n",
        "    print(f\"   Sources: {len(retrieved_docs)} relevant document chunks\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✅ System working successfully!\")\n",
        "print(\"💡 To get enhanced AI responses, add your API key in the setup section above.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
